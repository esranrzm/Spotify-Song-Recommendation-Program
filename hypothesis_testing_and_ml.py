# -*- coding: utf-8 -*-
"""hypothesis_testing_and_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u2-3BERhX7dnSE-joomChNW4_IrLOZdf

#Statistical Analysis & Hypothesis Testing

* At this step of our project, we will try to use some statistical methods to prove or disprove the null hypothesis.
* First we will examine the relationships between popularity of each song and the feature of them such as danceability, liveness, tempo, acousticness, instrumentalness etc.
* To be able to work efficiently we first sort the songs from ascending to descending order according to their popularity values.
* we will divide the data into two group. One is represent the most 15 popular song aand the others will represent the rest of the songs.

**Null Hypothesis ($H_0$)**: There is not significant differences in terms of popularity for different song features

**Alternative Hypothesis ($H_A$)**: There is significant differences in terms of popularity for different song features


**Significance level**: As most of hypothesis tests assume significance level as `0.05`, we are setting it as `0.05` for our test too.
"""

data_for_testing = first_data

data_for_testing.sort_values(by=['popularity'], inplace=True, ascending=False)

test_dataframe = data_for_testing.head(15)
other_dataframe = data_for_testing.tail(174374)

"""## Hypothesis 1
 * **Null Hypothesis ($H_0$)** = Danceability has no effect on the popularity of the songs
 * **Alternative Hypothesis ($H_A$)** = Danceability has an effect on the popularity of the song
"""

ax = sns.kdeplot(test_dataframe["danceability"].rename(""),shade=True)
sns.kdeplot(other_dataframe["danceability"].rename(""), ax=ax, shade= True)
plt.show()

"""* blue area represents the most popular songs danceability values and yellow area represents the other songs danceability values.
* Now we will examine p_value in order to find if our null hypothesis is true or false.
* our significance level will be 0.05
"""

significance_level = 0.05
test_dataframe_val = test_dataframe["danceability"]
other_dataframe_val = other_dataframe["danceability"]

_, p_value = stats.ttest_ind(a=test_dataframe_val, b=other_dataframe_val, equal_var=False)
print("P value: ",format(p_value,".2E"))

if(p_value < significance_level):
  print("According to p value that we calculated, we reject the null hypothesis which menas danceability has an effect on popularity of the song")
else:
  print("According to p value that we calculated, we fail to reject the null hypothesis")
  print("which means danceability has an effect on popularity of the song")

"""##Hypothesis 2
* **Null Hypothesis ($H_0$)** = tempo has no effect on the popularity of the songs
* **Alternative Hypothesis ($H_A$)** = tempo has an effect on the popularity of the song
"""

ax = sns.kdeplot(test_dataframe["tempo"].rename(""),shade=True)
sns.kdeplot(other_dataframe["tempo"].rename(""), ax=ax, shade= True)
plt.show()

significance_level = 0.05
test_dataframe_val = test_dataframe["tempo"]
other_dataframe_val = other_dataframe["tempo"]

_, p_value = stats.ttest_ind(a=test_dataframe_val, b=other_dataframe_val, equal_var=False)
print("P value: ",format(p_value,".2E"))
print("Signifance level: ",significance_level)

if(p_value < significance_level):
  print("According to p value that we calculated, we reject the null hypothesis which menas tempo has an effect on popularity of the song")
else:
  print("According to p value that we calculated, we fail to reject the null hypothesis which means tempo has an effect on popularity of the song")

"""##Hypothesis 3
* **Null Hypothesis ($H_0$)** = instrumentalness has no effect on the popularity of the songs
* **Alternative Hypothesis ($H_A$)** = instrumentalness has an effect on the popularity of the song
"""

ax = sns.kdeplot(test_dataframe["instrumentalness"].rename(""), shade=True)
sns.kdeplot(other_dataframe["instrumentalness"].rename(""), ax=ax, shade= True)
plt.show()

significance_level=0.05
test_dataframe_val = test_dataframe["instrumentalness"]
other_dataframe_val = other_dataframe["instrumentalness"]

_, p_value = stats.ttest_ind(a=test_dataframe_val, b=other_dataframe_val, equal_var=False)
print("P value: ",format(p_value,".2E"))
print("Signifance Level:", significance_level)
if (p_value < significance_level):
  print("According to p value that we calculated, we reject the null hypothesis which menas instrumentalness has an effect on popularity of the song")
else:
   print("According to p value that we calculated, we fail to reject the null hypothesis which means instrumentalness has an effect on popularity of the song")

"""##Hypothesis 4
* **Null Hypothesis ($H_0$)** = liveness has an effect on the popularity of the songs
* **Alternative Hypothesis ($H_A$)** = liveness has an effect on the popularity of the song 
"""

ax = sns.kdeplot(test_dataframe["liveness"].rename(""), shade=True)
sns.kdeplot(other_dataframe["liveness"].rename(""), ax=ax, shade= True)
plt.show()

significance_level=0.05
test_dataframe_val = test_dataframe["liveness"]
other_dataframe_val = other_dataframe["liveness"]

_, p_value = stats.ttest_ind(a=test_dataframe_val, b=other_dataframe_val, equal_var=False)
print("P value: ",format(p_value,".2E"))
print("Signifance Level:", significance_level)
if (p_value < significance_level):
  print("According to p value that we calculated, we reject the null hypothesis which menas liveness has an effect on popularity of the song")
else:
   print("According to p value that we calculated, we fail to reject the null hypothesis which means liveness has an effect on popularity of the song")

"""##Hypothesis 5
* **Null Hypothesis ($H_0$)** = acousticness has no effect on the popularity of the songs
* **Alternative Hypothesis ($H_A$)** = acousticness has an effect on the popularity of the song  
"""

ax = sns.kdeplot(test_dataframe["acousticness"].rename(""), shade=True)
sns.kdeplot(other_dataframe["acousticness"].rename(""), ax=ax, shade= True)
plt.show()

significance_level=0.05
test_dataframe_val = test_dataframe["acousticness"]
other_dataframe_val = other_dataframe["acousticness"]

_, p_value = stats.ttest_ind(a=test_dataframe_val, b=other_dataframe_val, equal_var=False)
print("P value: ",format(p_value,".2E"))
print("Signifance Level:", significance_level)
if (p_value < significance_level):
  print("According to p value that we calculated, we reject the null hypothesis which means acousticness has an effect on popularity of the song")
else:
   print("According to p value that we calculated, we fail to reject the null hypothesis which means acousticness has an effect on popularity of the song")

"""###Conclusion for hypothesis 1-2-3-4-5
  * We examined the relationships between the features of the songs and popularity.
  * We saw that almost all of the features has an effect on the popularity of the songs since we reject all null values
  * We conclude this result by comparing the p value of the features with the significance level 0.05

##Hypothesis 6
* **Null Hypothesis ($H_0$)** = 
No change in the danceability of the pop songs between  90s - 2000s
* **Alternative Hypothesis ($H_A$)** = Dancebility of the songs has changed between 90s - 2000s
"""

test_genre = genre_data1_2
other_genre = genre_data2_2

significance_level_g = 0.05
test_datagenre_val = test_genre["danceability_y"]
other_datagenre_val = other_genre["danceability_y"]

_, p_value = stats.ttest_ind(a=test_datagenre_val, b=other_datagenre_val, equal_var=False)
print("P value: ",format(p_value,".2E"))

if(p_value < significance_level_g):
  print("According to p value that we calculated, we reject the null hypothesis which means danceability level of pop songs has changed from 90s to 2000s")
else:
  print("According to p value that we calculated, we fail to reject the null hypothesis")
  print("which means danceability level of pop songs has not changed from 90s to 2000s")

"""##Hypothesis 7
* **Null Hypothesis ($H_0$)** = 
No change in the acousticness of the pop songs between  90s - 2000s
* **Alternative Hypothesis ($H_A$)** = Acousticness of the songs has changed between 90s - 2000s 
"""

significance_level_g = 0.05
test_datagenre_val = test_genre["acousticness_y"]
other_datagenre_val = other_genre["acousticness_y"]

_, p_value = stats.ttest_ind(a=test_datagenre_val, b=other_datagenre_val, equal_var=False)
print("P value: ",format(p_value,".2E"))

if(p_value < significance_level_g):
  print("According to p value that we calculated, we reject the null hypothesis which means acousticness level of pop songs has changed from 90s to 2000s")
else:
  print("According to p value that we calculated, we fail to reject the null hypothesis")
  print("which means acousticness level of pop songs has not changed from 90s to 2000s")

"""##Hypothesis 8
* **Null Hypothesis ($H_0$)** = 
No change in the instrumentalness of the pop songs between  90s - 2000s
* **Alternative Hypothesis ($H_A$)** = Instrumentalness of the songs has changed between 90s - 2000s 
"""

significance_level_g = 0.05
test_datagenre_val = test_genre["instrumentalness_y"]
other_datagenre_val = other_genre["instrumentalness_y"]

_, p_value = stats.ttest_ind(a=test_datagenre_val, b=other_datagenre_val, equal_var=False)
print("P value: ",format(p_value,".2E"))

if(p_value < significance_level_g):
  print("According to p value that we calculated, we reject the null hypothesis which means instrumentalness level of pop songs has changed from 90s to 2000s")
else:
  print("According to p value that we calculated, we fail to reject the null hypothesis")
  print("which means instrumentalness level of pop songs has not changed from 90s to 2000s")

"""###Conclusion for hypothesis 6-7-8

* We examine the changes on fetures of pop music between 90s and 2000s
* In order to compare two eras we used t-test and calculate the p-value of the samples
* According to the p-values we reject or fail to reject the null hypothesis and came to a conclusion
* At the end, we found that there are some changes occured in pop music in terms of features (such as danceability, instrumentalness, and acousticness)  between 90s and 2000s

#Machine Learning

##Prediction of song popularity with various machine learning models
* For this purpose we will use the following prediction models
  
    * Logistic regression
    * KNN
    * Random forest
    * Decision trees

* To make a classification, we set a popularity level to determine whether a song is popular enough or not.
* To do this, our bound will be 60. Which means if the popularity level of a song is lower that 60, then it will be considered as not popular enough and if its popularity level is above 60, then that song will considered as a popular song
"""

df = first_data

df.loc[df['popularity'] < 55, 'popularity'] = 0 
df.loc[df['popularity'] >= 55, 'popularity'] = 1

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import make_scorer, accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

attributes = ["acousticness", "danceability", "duration_ms", "energy", "instrumentalness", "key", "liveness", 
            "mode", "speechiness", "tempo","valence"]

training_sample = df.sample(frac = 0.6,random_state = 0)
X_train = training_sample[attributes]
y_train = training_sample['popularity']
X_test = df.drop(training_sample.index)[attributes]

X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.4, random_state = 0)

"""### Test for logistic regression to calculate accuracy level"""

logisticReg_Model = LogisticRegression()
logisticReg_Model.fit(X_train, y_train)
logisticReg_Predict = logisticReg_Model.predict(X_valid)
logisticReg_Accuracy = accuracy_score(y_valid, logisticReg_Predict)
print("Accuracy: " + str(logisticReg_Accuracy))

"""### Test for KNN to calculate accuracy level"""

KNN_Model = KNeighborsClassifier()
KNN_Model.fit(X_train, y_train)
KNN_Predict = KNN_Model.predict(X_valid)
KNN_Accuracy = accuracy_score(y_valid, KNN_Predict)
print("Accuracy: " + str(KNN_Accuracy))

"""### Test for decision Tree to calculate accuracy level"""

decisionTree_Model = DecisionTreeClassifier()
decisionTree_Model.fit(X_train, y_train)
decisionTree_Predict = decisionTree_Model.predict(X_valid)
decisionTree_Accuracy = accuracy_score(y_valid, decisionTree_Predict)
print("Accuracy: " + str(decisionTree_Accuracy))

"""### Test for random forest to calculate accuracy level"""

randomForest_Model = RandomForestClassifier()
randomForest_Model.fit(X_train, y_train)
randomForest_Predict = randomForest_Model.predict(X_valid)
randomForest_Accuracy = accuracy_score(y_valid, randomForest_Predict)
print("Accuracy: " + str(randomForest_Accuracy))

"""* Let's compare the accuracy levels of each test"""

model_performance_accuracy = pd.DataFrame({'Prediction Model': ['Logistic Regression','Random Forest Classifier', 'KNN Classifier','Decision Tree Classifier'],
                                            'Accuracy': [logisticReg_Accuracy,randomForest_Accuracy,KNN_Accuracy,decisionTree_Accuracy]})

model_performance_accuracy.sort_values(by = "Accuracy", ascending = False)

"""#### Conclusion

* As you can see from the table Random Forest Classifier gives us the best accuracy level to predict the popularity of the song
* But other test also give us a good level of accuracy level as well
* Our best accuracy level is 0.893716

## Efforts on hyper-parameter tuning to increase the performance of models

* We will use decision tree model as an example
* As we did in the recitation, we will play with the max_depth of our tree to increase the performance of our model
"""

decisionTree_Model.get_params()

max_depth_values = np.arange(1, 15)

acc_training_scores = []
acc_val_scores = []

for max_depth in max_depth_values:
  model = DecisionTreeClassifier(max_depth=max_depth)
  model.fit(X_train,y_train)

  y_pred_training = model.predict(X_train)
  y_pred_val = model.predict(X_valid)

  acc_training = accuracy_score(y_train, y_pred_training)
  acc_val = accuracy_score(y_valid, y_pred_val)

  acc_training_scores.append(acc_training)
  acc_val_scores.append(acc_val)

"""* Now, our max_depth value is changed from none to a number
* From now on we can set its value
"""

model.get_params()

model = DecisionTreeClassifier(max_depth=4)

model.fit(X_train, y_train)

from sklearn.model_selection import cross_val_score

accuracies = cross_val_score(model, X_train, y_train, cv=5, scoring="accuracy")
accuracies

"Accuracy: {:.2f} (+/- {:.2f})".format(accuracies.mean(), accuracies.std() * 2)

"""* With hyper-parameter tuning, we incresed the performance of our model and we got a better accuracy value which is 0.88"""

# tree plot function
from sklearn.tree import plot_tree

# generate the tree
fig = plt.figure(figsize=(35, 15))
plot_tree(model, feature_names=X_test.columns.values, class_names=["not popular", "popular"], filled=True);